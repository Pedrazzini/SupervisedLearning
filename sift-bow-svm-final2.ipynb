{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8802726,"sourceType":"datasetVersion","datasetId":5293846}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import cv2\n","from sklearn.cluster import KMeans\n","import pickle\n","from scipy.spatial.distance import cdist\n","import os\n","import glob\n"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"34OHiuBwVUPF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","import torchvision.datasets\n","from torch.utils.data import DataLoader, Subset\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"trusted":true,"id":"7exEq91WVUPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read dataset\n","trainLabels = pd.read_csv('/kaggle/input/supervised-sets/train_labels.csv')\n","testLabels = pd.read_csv('//kaggle/input/supervised-sets/val_labels.csv')\n","\n","trainClasses = trainLabels['label'].unique()\n","testClasses = testLabels['label'].unique()"],"metadata":{"trusted":true,"id":"Ymvij0R9VUPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dir = '/kaggle/input/supervised-sets/processedData/processedData/processed_train_set'\n","val_dir = '/kaggle/input/supervised-sets/processedData/processedData/processed_val_set' # test directory"],"metadata":{"trusted":true,"id":"pv7Jyw7IVUPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["size = 224\n","\n","fake_transforms = torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Resize((size, size))\n","    ])\n","\n","\n","# creation of the training set of resized images\n","fake_training_set = torchvision.datasets.ImageFolder(root='/kaggle/input/supervised-sets/processedData/processedData/processed_train_set', transform=fake_transforms)\n","# divide the images in batches: \"fake\" loader in order to compute mean and std for normalization\n","fake_train_loader = DataLoader(fake_training_set, batch_size=64, shuffle=True, num_workers=4)"],"metadata":{"trusted":true,"id":"-YeWIJU_VUPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute the mean and standard deviation for the normalization\n","def tot_mean_std(loader):\n","    mean = 0\n","    std = 0\n","    count = 0\n","    for batch, _ in loader:\n","        batch_samples = batch.size(0)\n","        batch = batch.view(batch_samples, -1)\n","        mean += batch.mean(1).sum(0)  # mean over the pixels of each image of the batch summed to the others\n","        std += batch.std(1).sum(0)  # same for the standard deviation\n","        count += batch_samples\n","\n","    mean /= count\n","    std /= count\n","\n","    return mean, std"],"metadata":{"trusted":true,"id":"TEczFR6yVUPM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["size = 224\n","mean, std = tot_mean_std(fake_train_loader)\n","\n","transforms = torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Resize((size, size)),\n","        torchvision.transforms.Normalize(mean = mean, std = std),\n","        torchvision.transforms.Grayscale()\n","    ])\n","\n","training_set = torchvision.datasets.ImageFolder(root='/kaggle/input/supervised-sets/processedData/processedData/processed_train_set', transform=transforms)\n","test_set = torchvision.datasets.ImageFolder(root='/kaggle/input/supervised-sets/processedData/processedData/processed_test_set', transform=transforms)\n","\n","\n","# get 30% from any classes to reduce the dimension of the dataset\n","def get_subset(dataset, subset_percentage):\n","    class_indices = {}\n","    for idx, (_, label) in enumerate(dataset.samples):\n","        if label not in class_indices:\n","            class_indices[label] = []\n","        class_indices[label].append(idx)\n","\n","    subset_indices = []\n","    for label, indices in class_indices.items():\n","        np.random.shuffle(indices)\n","        n_subset = int(len(indices) * subset_percentage)\n","        subset_indices.extend(indices[:n_subset])\n","\n","    return Subset(dataset, subset_indices)\n","\n","# get the subset\n","subset_dataset = get_subset(training_set, subset_percentage=0.3)\n","\n","\n","\n","# normalized \"true\" dataloaders\n","\n","train_loader = DataLoader(subset_dataset, batch_size=64, shuffle=True, num_workers=4)\n","test_loader = DataLoader(test_set, batch_size=64, shuffle=True, num_workers=4)"],"metadata":{"trusted":true,"id":"8mLaa9lrVUPN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(mean)\n","print(std)"],"metadata":{"trusted":true,"id":"N8r-OsUcVUPO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check if the images in the subset corresponds to their class\n","def imshow(img):\n","    img = img / 2 + 0.5\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","# iter for only two batches\n","for i, data in enumerate(train_loader, 0):\n","    if i >= 2:  # Fermarsi dopo 2 batch\n","        break\n","\n","    data_train, train_labels = data\n","\n","    # print images and labels\n","    for j in range(len(data_train)):\n","        print(f\"Etichetta: {training_set.classes[train_labels[j]]}\")\n","        imshow(data_train[j])\n","        # stop\n","        break"],"metadata":{"trusted":true,"id":"rPSIM_GCVUPO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(subset_dataset)"],"metadata":{"trusted":true,"id":"aCXwW8P_VUPP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(training_set)"],"metadata":{"trusted":true,"id":"DlEX3ia2VUPQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to extract sift features\n","def sift_features(image_list):\n","    descriptors = []\n","    valid_indices = []\n","    sift = cv2.SIFT_create()\n","    for i, image in enumerate(image_list):\n","        if image is not None:\n","            image_np = image.numpy().transpose(1, 2, 0).astype(np.uint8)\n","            _, descriptor = sift.detectAndCompute(image_np, None)\n","            if descriptor is not None:\n","                descriptors.append(descriptor)\n","                valid_indices.append(i)\n","        else:\n","            print(\"Null image\")\n","\n","    return descriptors, valid_indices"],"metadata":{"trusted":true,"id":"7I_BYQ9jVUPQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create BOW dictionary with k-means applied on SIFT descriptors to compute centroids\n","# G is the number of words in the vocaboulary\n","def bow_dictionary(descriptors, G):\n","    bow_dict = []\n","\n","    kmeans = KMeans(n_clusters = G)\n","    kmeans.fit(descriptors)\n","\n","    bow_dict = kmeans.cluster_centers_\n","\n","    if not os.path.isfile('bow_dictionary.pkl'):\n","        pickle.dump(bow_dict, open('bow_dictionary.pkl', 'wb'))\n","\n","    return bow_dict"],"metadata":{"trusted":true,"id":"yojEIqqbVUPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the histogram of bow features\n","def bow_features(descriptors, centers, G):\n","    bow_features = []\n","    for descriptor_set in descriptors:\n","        for descriptor in descriptor_set:\n","            if descriptor is not None:\n","                features = np.zeros(G)\n","                distance = cdist(descriptor, centers)\n","                minimum = np.argmin(distance, axis=1)\n","                for index in minimum:\n","                    features[index] += 1\n","                bow_features.append(features)\n","            else:\n","                print(\"Null descriptor\")\n","    return bow_features"],"metadata":{"trusted":true,"id":"lWJoVnA-VUPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract descriptors\n","import sklearn\n","import argparse\n","from sklearn.model_selection import GridSearchCV\n","\n","all_train_descriptors = []\n","training_descriptors = []\n","training_labels = []\n","valid_indices_all = []\n","\n","for i, data in enumerate(train_loader, 0):\n","    data_train, train_labels = data\n","    train_descriptors, valid_indices = sift_features(data_train)\n","    training_descriptors.append(train_descriptors)\n","    training_labels.extend(train_labels[valid_indices])\n","    valid_indices_all.extend(valid_indices)\n","    for descriptor in train_descriptors:\n","        if descriptor is not None:\n","            all_train_descriptors.extend(descriptor)"],"metadata":{"trusted":true,"id":"gu4RayRJVUPS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(valid_indices_all))"],"metadata":{"trusted":true,"id":"8clPJdVaVUPS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(training_labels))"],"metadata":{"trusted":true,"id":"DDtBXukxVUPS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create dictionary, and bow_features\n","\n","G = 380\n","bow_dict_train = bow_dictionary(all_train_descriptors, G)\n","train_features = bow_features(training_descriptors, bow_dict_train, G)\n"],"metadata":{"trusted":true,"id":"MMKSEnBJVUPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svm_model = sklearn.svm.SVC(C = 30, random_state = 0)"],"metadata":{"trusted":true,"id":"Ok8yhKOnVUPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_labels = [tensor.item() for tensor in training_labels]"],"metadata":{"trusted":true,"id":"bEXzL9KFVUPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(training_labels[:10])  # print the firste 10 values as example"],"metadata":{"trusted":true,"id":"45O26SSwVUPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y_train = training_labels"],"metadata":{"trusted":true,"id":"PLoczPkzVUPU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fit the model (SVM)\n","svm_model.fit(train_features, Y_train)\n","filename = 'svm_model.sav'\n","pickle.dump(svm_model, open(filename, 'wb'))"],"metadata":{"trusted":true,"id":"FISNPSfhVUPU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(Y_train))"],"metadata":{"trusted":true,"id":"vKdclTQ9VUPU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_features))"],"metadata":{"trusted":true,"id":"YL2H2jerVUPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat for the test set\n","\n","all_test_descriptors = []\n","testing_descriptors = []\n","testing_labels = []\n","valid_indices_all_test = []\n","\n","for i, data in enumerate(test_loader, 0):\n","    data_test, test_labels = data\n","    test_descriptors, valid_indices_test = sift_features(data_test)\n","    testing_descriptors.append(test_descriptors)\n","    testing_labels.extend(test_labels[valid_indices_test])\n","    valid_indices_all_test.extend(valid_indices_test)\n","    for test_descriptor in test_descriptors:\n","        if test_descriptor is not None:\n","            all_test_descriptors.extend(test_descriptor)\n","\n","\n","\n","test_features = bow_features(testing_descriptors, bow_dict_train, G) # attention: the dictionary is the same of the training set\n","testing_labels = [tensor.item() for tensor in testing_labels]\n","Y_test = testing_labels\n","# do predictions using SVM\n","predictions = svm_model.predict(test_features)\n","\n","# compute accuracy\n","accuracy = sklearn.metrics.accuracy_score(Y_test, predictions)\n","print(f\"Accuracy on test set: {accuracy:.4f}\")"],"metadata":{"trusted":true,"id":"XdU-noZvVUPV"},"execution_count":null,"outputs":[]}]}